{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDm1VJQmtqZQMdBd+05LC0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carsondenison/proximal-policy-optimization/blob/main/PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWDmBhbbgCOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c224943f-fc43-4ca4-8752-0061f1f28a1c"
      },
      "source": [
        "!pip install --quiet \"torch\" \"pytorch-lightning\" \"gym\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 525 kB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 332 kB 61.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 33.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 67.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 76.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 48.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 73.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 60.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 192 kB 62.9 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBEWS00tg-N-"
      },
      "source": [
        "Step 0: Import the libraries we'll need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMJD_V7Xg8Cw"
      },
      "source": [
        "import random\n",
        "from typing import List, Tuple, Iterable\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# Implements: https://arxiv.org/pdf/1707.06347.pdf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR-76LzFrP8a"
      },
      "source": [
        "Step 1: Create a dataset to store experiences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DREoneklrsDp"
      },
      "source": [
        "Experience = namedtuple('Experience', 'state action reward done new_state')\n",
        "\n",
        "class ReplayBuffer():\n",
        "    '''\n",
        "        Buffer to hold Experiences for training\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.buffer:List[Experience] = []\n",
        "    \n",
        "    def append(self, x):\n",
        "        self.buffer.append(x)\n",
        "    \n",
        "    def clear(self):\n",
        "        self.buffer = []\n",
        "\n",
        "    def to_batch(self) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
        "        states, actions, rewards, dones, new_states = zip(*self.buffer)\n",
        "        states = torch.tensor(states, dtype=torch.float32, requires_grad=True)\n",
        "        actions = torch.tensor(actions, dtype=torch.int64)[:, None]\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)[:, None]\n",
        "        dones = torch.tensor(dones, dtype=torch.bool)[:, None]\n",
        "        new_states = torch.tensor(new_states, dtype=torch.float32)\n",
        "        return states, actions, rewards, dones, new_states"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Y0Jq51syeH"
      },
      "source": [
        "Step 2: Create an actor that can interact with the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T7CGGbDs2RP"
      },
      "source": [
        "class Actor():\n",
        "    '''\n",
        "        Class which can interact with the environment\n",
        "    '''\n",
        "    def __init__(self, env:gym.Env, replay_buffer:ReplayBuffer, pi:nn.Module):\n",
        "        self.env = env\n",
        "        self.buffer = buffer\n",
        "        self.pi = pi\n",
        "        self.state = self.env.reset() # self.state is a numpy array\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer.clear()\n",
        "        self.state = self.env.reset()\n",
        "\n",
        "    def get_action(self) -> int:\n",
        "        '''\n",
        "            Samples the policy to get an action given self.state\n",
        "        '''\n",
        "        pi_logits = self.pi(torch.tensor(self.state))\n",
        "        policy = torch.distributions.categorical.Categorical(logits=pi_logits)\n",
        "        action = policy.sample()\n",
        "        return action.item()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play_step(self) -> None:\n",
        "        '''\n",
        "            Play one step of the environment, and add it to the buffer\n",
        "        '''\n",
        "        action = self.get_action()\n",
        "        new_state, reward, done, _ = self.env.step(action)\n",
        "        exp = Experience(self.state, action, reward, done, new_state)\n",
        "        self.buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if done:\n",
        "            self.state = self.env.reset()\n",
        "        return done\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJbFYw2r0rUv"
      },
      "source": [
        "Step 3: Define the neural network architecture for policy and advantage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnF_oxY20xNa"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    '''\n",
        "        Simple MLP, as described in https://arxiv.org/pdf/1707.06347.pdf\n",
        "    '''\n",
        "    def __init__(self, in_size, out_size, hidden_size=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, out_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.net(state.float())"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAgnACB_ZJYO"
      },
      "source": [
        "Step 4: Define an advantage estimator function. This is built from a value network and a reward_to_go calculator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc8NakkUZQIc"
      },
      "source": [
        "def reward_to_go(rewards:Tensor, dones:Tensor, gamma:float) -> Tensor:\n",
        "    '''\n",
        "        Calculates the rewards_to_go for a trajectory\n",
        "\n",
        "        Args:\n",
        "            rewards: (T, 1) float32 of rewards for each step\n",
        "            dones: (T, 1) bool - if state i was terminal\n",
        "            gamma: discount factor for each step\n",
        "        Returns:\n",
        "            rewards_to_go: Discounted reward_to_go for each reward in rewards\n",
        "    '''\n",
        "    rewards_to_go = torch.zeros_like(rewards, dtype=torch.float32)\n",
        "    for i in reversed(range(len(dones))):\n",
        "        if dones[i] or i == len(dones) - 1:\n",
        "            rewards_to_go[i] = rewards[i]\n",
        "        else:\n",
        "            rewards_to_go[i] = rewards[i] + gamma * rewards_to_go[i + 1]\n",
        "    return rewards_to_go  \n",
        "\n",
        "# quick tests for reward_to_go\n",
        "dones_3 = torch.tensor([0,0,0], dtype=bool)\n",
        "dones_1 = torch.tensor([0], dtype=bool)\n",
        "\n",
        "rtg = reward_to_go(torch.tensor([1, 1, 1,]), dones_3, 1)\n",
        "assert torch.all(torch.eq(rtg, torch.tensor([3, 2, 1]))), str(rtg)\n",
        "rtg2 = reward_to_go(torch.tensor([1, 1, 1]), dones_3, 0.5)\n",
        "assert torch.all(torch.eq(rtg2, torch.tensor([1.75, 1.5, 1]))), str(rtg2)\n",
        "rtg3 = reward_to_go(torch.tensor([]), torch.tensor([]), 1)\n",
        "assert torch.all(torch.eq(rtg3, torch.tensor([])))\n",
        "rtg4 = reward_to_go(torch.tensor([1]), dones_1, 0.5)\n",
        "assert torch.eq(rtg4, torch.tensor(1)), str(rtg4)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BiCR784255h"
      },
      "source": [
        "def estimate_advantage(states:Tensor, rewards:Tensor, dones:Tensor, value_net:nn.Module, gamma:float, final_state:Tensor=None) -> Tensor:\n",
        "    '''\n",
        "        Compute advantage estimate for each step in a trajectory\n",
        "\n",
        "        Args:\n",
        "            rewards: (T, 1) torch.float32 - Reward given by each step in the trajectory\n",
        "            states: (T, state_size) torch.float32 - observation vectors for each state\n",
        "            dones: (T, 1) bool - if state was terminal\n",
        "            v_net: Trainable network which predicts the value V(s) of a state\n",
        "            gamma: Discount factor. Assume lambda = 1 from GAE-Lambda\n",
        "            final_state: if given final_state, dones[-1] must be equal to 0\n",
        "        Returns:\n",
        "            advantages: The estimated advantage for each step in the trajectory\n",
        "    '''\n",
        "\n",
        "    values = value_net(states) # Shape (T, 1)\n",
        "    values.masked_fill_(dones, 0)\n",
        "    rewards_to_go = reward_to_go(rewards, dones, gamma)\n",
        "    discounted_final_values = torch.zeros_like(values, dtype=torch.float32)\n",
        "    if final_state is not None:\n",
        "        final_value = value_net(final_state).item()\n",
        "        discount = gamma\n",
        "        for t in reversed(range(len(dones))):\n",
        "            if dones[t]:\n",
        "                break\n",
        "            discounted_final_values[t] = discount * final_value \n",
        "            discount *= gamma\n",
        "    advantages = rewards_to_go - values + discounted_final_values\n",
        "    return advantages"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "    This cell just holds some tests for the advantage_estimator() function\n",
        "'''\n",
        "class FakeValue(nn.Module):\n",
        "    '''\n",
        "        torch.nn.Module that returns the identiy. Useful for testing\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "fake_value_net = FakeValue()\n",
        "r = torch.tensor([1,1])\n",
        "s = torch.tensor([1,1])\n",
        "d = torch.tensor([0,0])\n",
        "\n",
        "# Test that discounting of values works with gamma = 1, done = True\n",
        "adv = estimate_advantage(s, r, d, fake_value_net, 1)\n",
        "assert torch.allclose(adv, torch.tensor([1,0], dtype=torch.float32)), adv\n",
        "# Test that discounting of values works with gamma = 0.3, done = True\n",
        "adv2 = estimate_advantage(s, r, d, fake_value_net, 0.3)\n",
        "assert torch.allclose(adv2, torch.tensor([0.3,0], dtype=torch.float32)), adv2\n",
        "# Test that this works with a final_state\n",
        "final_state = torch.tensor([2])\n",
        "adv3 = estimate_advantage(s, r, d, fake_value_net, 0.5, final_state)\n",
        "assert torch.allclose(adv3, torch.tensor([1,1], dtype=torch.float32)), adv3\n",
        "\n",
        "# Test that this works with intermediate dones:\n",
        "r = torch.tensor([1,1,0,1,1,0])\n",
        "s = torch.tensor([2,2,1,2,2,1])\n",
        "d = torch.tensor([0,0,1,0,0,1])\n",
        "adv4 = estimate_advantage(s, r, d, fake_value_net, 0.5, final_state=None)\n",
        "expected = torch.tensor([-0.5, -1, 0, -0.5, -1, 0])\n",
        "assert torch.allclose(adv4, expected), adv4"
      ],
      "metadata": {
        "id": "hVYMlPxmJSD_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikqk4irB2AIG"
      },
      "source": [
        "Step 5: Define the clipped loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAIH_Kur4gzl"
      },
      "source": [
        "def clipped_loss(states:Tensor, actions:Tensor, advantages:Tensor, pi_old:Tensor, pi_net:nn.Module, epsilon=0.2) -> Tensor:\n",
        "    '''\n",
        "        PPO Clipped Loss\n",
        "\n",
        "        Args:\n",
        "            states: The states from a given trajectory T\n",
        "            advantages: Advantage estimates for T, based on GAE-Lambda\n",
        "            pi_old: Probability distribution for pi(a|s) used to generate the trajectory\n",
        "            pi_net: Most up to date policy network\n",
        "            epsilon: Clipping hyperparameter for loss. See page 3 - https://arxiv.org/pdf/1707.06347.pdf\n",
        "        Returns:\n",
        "            loss: L_clip used to optimize the policy network\n",
        "    '''\n",
        "    pi = pi_net(states).gather(1, actions)\n",
        "    ratio = torch.div(pi, pi_old)\n",
        "    unclipped = ratio * advantages\n",
        "    clipped = torch.clamp(ratio, 1-epsilon, 1+epsilon) * advantages\n",
        "    elementwise_mins = torch.minimum(unclipped, clipped)\n",
        "    loss = -1 * torch.mean(elementwise_mins)\n",
        "    return loss"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV6r7gklSAho"
      },
      "source": [
        "Step 6: Main Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtUZi_RW-10N",
        "outputId": "d3350745-8e0c-429b-f02a-4f16078258a9"
      },
      "source": [
        "# Set up the environment\n",
        "env = gym.make('CartPole-v0')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Build the neural networks and optimizers\n",
        "pi = MLP(state_size, action_size)\n",
        "v = MLP(state_size, 1)\n",
        "\n",
        "pi_optimizer = torch.optim.Adam(pi.parameters(), lr=1e-3)\n",
        "v_optimizer = torch.optim.Adam(v.parameters(), lr=1e-3)\n",
        "v_loss_fn = nn.MSELoss()\n",
        "\n",
        "# Set up the buffer and actor\n",
        "buffer = ReplayBuffer()\n",
        "actor = Actor(env, buffer, pi)\n",
        "\n",
        "# Hyperparameters\n",
        "total_epochs = 201\n",
        "episode = 0\n",
        "longest_episode_length = 0\n",
        "gamma = 1\n",
        "K_pi = 10\n",
        "K_v = 10\n",
        "minibatch_size = 10\n",
        "explore_steps = 1000\n",
        "\n",
        "\n",
        "for epoch in range(total_epochs):\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'epoch: {epoch}')\n",
        "\n",
        "    # Play steps to generate on-policy data\n",
        "    episode_length = 0\n",
        "    episodes_this_epoch = 0\n",
        "    for _ in range(explore_steps):\n",
        "        done = actor.play_step()\n",
        "        episode_length += 1\n",
        "        if done:\n",
        "            episode += 1\n",
        "            episodes_this_epoch += 1\n",
        "            # Log eisode information to the console\n",
        "            if episode_length >= longest_episode_length:\n",
        "                longest_episode_length = episode_length\n",
        "                #print(f'epoch: {epoch} episode: {episode} length: {longest_episode_length}')\n",
        "            episode_length = 0\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        print(f'average episode length: {explore_steps / episodes_this_epoch}')\n",
        "\n",
        "\n",
        "    # Unpack replay buffer into arrays and reset the buffer\n",
        "    states, actions, rewards, dones, new_states = buffer.to_batch()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'nonzero actions: {actions.count_nonzero()}')\n",
        "    actor.reset()\n",
        "\n",
        "    # Compute the information we'll need to feed the two loss functions\n",
        "    if done:\n",
        "        final_state = None\n",
        "    else:\n",
        "        final_state = new_states[-1]\n",
        "    advantages = estimate_advantage(states, rewards, dones, v, gamma, final_state).detach()\n",
        "    pi_old = pi(states).gather(1, actions).detach()\n",
        "    rtg = reward_to_go(rewards, dones, gamma)\n",
        "\n",
        "    # Update policy network for K steps\n",
        "    # A full implementation have a KL divergence check here to stop updating policy network and get another trajectory when the policy changes too much\n",
        "    for _ in range(K_pi):\n",
        "        for i in range(0, len(states), minibatch_size):\n",
        "            pi_optimizer.zero_grad()\n",
        "            pi_loss = clipped_loss(states[i:i+minibatch_size], actions[i:i+minibatch_size], advantages[i:i+minibatch_size], pi_old[i:i+minibatch_size], pi)\n",
        "            pi_loss.backward()\n",
        "            pi_optimizer.step()\n",
        "\n",
        "    # Update Value Network V(s_t) should be reward_to_go_t. MSE Loss\n",
        "    for _ in range(K_v):\n",
        "        for i in range(0, len(states), minibatch_size):\n",
        "            values = v(states[i:i+minibatch_size])\n",
        "            v_optimizer.zero_grad()\n",
        "            v_loss = v_loss_fn(values, rtg[i:i+minibatch_size])\n",
        "            v_loss.backward()\n",
        "            v_optimizer.step()    \n",
        "    #print(f'v loss: {v_loss}')\n",
        "    #print(f'mean squared v: {torch.mean(torch.square(values))}')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "average episode length: 21.27659574468085\n",
            "nonzero actions: 512\n",
            "epoch: 10\n",
            "average episode length: 27.027027027027028\n",
            "nonzero actions: 527\n",
            "epoch: 20\n",
            "average episode length: 30.303030303030305\n",
            "nonzero actions: 500\n",
            "epoch: 30\n",
            "average episode length: 30.303030303030305\n",
            "nonzero actions: 477\n",
            "epoch: 40\n",
            "average episode length: 100.0\n",
            "nonzero actions: 508\n",
            "epoch: 50\n",
            "average episode length: 166.66666666666666\n",
            "nonzero actions: 526\n",
            "epoch: 60\n",
            "average episode length: 142.85714285714286\n",
            "nonzero actions: 507\n",
            "epoch: 70\n",
            "average episode length: 166.66666666666666\n",
            "nonzero actions: 489\n",
            "epoch: 80\n",
            "average episode length: 200.0\n",
            "nonzero actions: 505\n",
            "epoch: 90\n",
            "average episode length: 111.11111111111111\n",
            "nonzero actions: 495\n",
            "epoch: 100\n",
            "average episode length: 200.0\n",
            "nonzero actions: 500\n",
            "epoch: 110\n",
            "average episode length: 200.0\n",
            "nonzero actions: 500\n",
            "epoch: 120\n",
            "average episode length: 200.0\n",
            "nonzero actions: 494\n",
            "epoch: 130\n",
            "average episode length: 111.11111111111111\n",
            "nonzero actions: 522\n",
            "epoch: 140\n",
            "average episode length: 35.714285714285715\n",
            "nonzero actions: 472\n",
            "epoch: 150\n",
            "average episode length: 166.66666666666666\n",
            "nonzero actions: 534\n",
            "epoch: 160\n",
            "average episode length: 43.47826086956522\n",
            "nonzero actions: 540\n",
            "epoch: 170\n",
            "average episode length: 142.85714285714286\n",
            "nonzero actions: 528\n",
            "epoch: 180\n",
            "average episode length: 200.0\n",
            "nonzero actions: 502\n",
            "epoch: 190\n",
            "average episode length: 166.66666666666666\n",
            "nonzero actions: 482\n",
            "epoch: 200\n",
            "average episode length: 111.11111111111111\n",
            "nonzero actions: 471\n"
          ]
        }
      ]
    }
  ]
}