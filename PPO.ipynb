{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhQojvP8j1AE53qD94awP5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carsondenison/proximal-policy-optimization/blob/main/PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWDmBhbbgCOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c224943f-fc43-4ca4-8752-0061f1f28a1c"
      },
      "source": [
        "!pip install --quiet \"torch\" \"pytorch-lightning\" \"gym\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 525 kB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 332 kB 61.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 33.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 67.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 76.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 48.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 73.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 60.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 192 kB 62.9 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBEWS00tg-N-"
      },
      "source": [
        "Step 0: Import the libraries we'll need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMJD_V7Xg8Cw"
      },
      "source": [
        "import random\n",
        "from typing import List, Tuple, Iterable\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# Implements: https://arxiv.org/pdf/1707.06347.pdf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR-76LzFrP8a"
      },
      "source": [
        "Step 1: Create a dataset to store experiences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DREoneklrsDp"
      },
      "source": [
        "Experience = namedtuple('Experience', 'state action reward done new_state')\n",
        "\n",
        "class ReplayBuffer():\n",
        "    '''\n",
        "        Buffer to hold Experiences for training\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.buffer:List[Experience] = []\n",
        "    \n",
        "    def append(self, x):\n",
        "        self.buffer.append(x)\n",
        "    \n",
        "    def clear(self):\n",
        "        self.buffer = []\n",
        "\n",
        "    def to_batch(self) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
        "        states, actions, rewards, dones, new_states = zip(*self.buffer)\n",
        "        states = torch.tensor(states, dtype=torch.float32, requires_grad=True)\n",
        "        actions = torch.tensor(actions, dtype=torch.int64)[:, None]\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)[:, None]\n",
        "        dones = torch.tensor(dones, dtype=torch.bool)[:, None]\n",
        "        new_states = torch.tensor(new_states, dtype=torch.float32)\n",
        "        return states, actions, rewards, dones, new_states"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Y0Jq51syeH"
      },
      "source": [
        "Step 2: Create an actor that can interact with the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T7CGGbDs2RP"
      },
      "source": [
        "class Actor():\n",
        "    '''\n",
        "        Class which can interact with the environment\n",
        "    '''\n",
        "    def __init__(self, env:gym.Env, replay_buffer:ReplayBuffer, pi:nn.Module):\n",
        "        self.env = env\n",
        "        self.buffer = buffer\n",
        "        self.pi = pi\n",
        "        self.state = self.env.reset() # self.state is a numpy array\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer.clear()\n",
        "        self.state = self.env.reset()\n",
        "\n",
        "    def get_action(self) -> int:\n",
        "        '''\n",
        "            Samples the policy to get an action given self.state\n",
        "        '''\n",
        "        pi_logits = self.pi(torch.tensor(self.state))\n",
        "        policy = torch.distributions.categorical.Categorical(logits=pi_logits)\n",
        "        action = policy.sample()\n",
        "        return action.item()\n",
        "        \n",
        "    @torch.no_grad()\n",
        "    def play_step(self) -> None:\n",
        "        '''\n",
        "            Play one step of the environment, and add it to the buffer\n",
        "        '''\n",
        "        action = self.get_action()\n",
        "        new_state, reward, done, _ = self.env.step(action)\n",
        "        exp = Experience(self.state, action, reward, done, new_state)\n",
        "        self.buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if done:\n",
        "            self.state = self.env.reset()\n",
        "        return done\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJbFYw2r0rUv"
      },
      "source": [
        "Step 3: Define the neural network architecture for policy and advantage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnF_oxY20xNa"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    '''\n",
        "        Simple MLP, as described in https://arxiv.org/pdf/1707.06347.pdf\n",
        "    '''\n",
        "    def __init__(self, in_size:int, out_size:int, hidden_size:int=64, output_scale:int=1):\n",
        "        super().__init__()\n",
        "        self.output_scale = output_scale\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, out_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.output_scale * self.net(state.float())"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAgnACB_ZJYO"
      },
      "source": [
        "Step 4: Define an advantage estimator function. This is built from a value network and a reward_to_go calculator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc8NakkUZQIc"
      },
      "source": [
        "def reward_to_go(rewards:Tensor, dones:Tensor, gamma:float) -> Tensor:\n",
        "    '''\n",
        "        Calculates the rewards_to_go for a trajectory\n",
        "\n",
        "        Args:\n",
        "            rewards: (T, 1) float32 of rewards for each step\n",
        "            dones: (T, 1) bool - if state i was terminal\n",
        "            gamma: discount factor for each step\n",
        "        Returns:\n",
        "            rewards_to_go: Discounted reward_to_go for each reward in rewards\n",
        "    '''\n",
        "    rewards_to_go = torch.zeros_like(rewards, dtype=torch.float32)\n",
        "    for i in reversed(range(len(dones))):\n",
        "        if dones[i] or i == len(dones) - 1:\n",
        "            rewards_to_go[i] = rewards[i]\n",
        "        else:\n",
        "            rewards_to_go[i] = rewards[i] + gamma * rewards_to_go[i + 1]\n",
        "    return rewards_to_go  \n",
        "\n",
        "# quick tests for reward_to_go\n",
        "dones_3 = torch.tensor([0,0,0], dtype=bool)\n",
        "dones_1 = torch.tensor([0], dtype=bool)\n",
        "\n",
        "rtg = reward_to_go(torch.tensor([1, 1, 1,]), dones_3, 1)\n",
        "assert torch.all(torch.eq(rtg, torch.tensor([3, 2, 1]))), str(rtg)\n",
        "rtg2 = reward_to_go(torch.tensor([1, 1, 1]), dones_3, 0.5)\n",
        "assert torch.all(torch.eq(rtg2, torch.tensor([1.75, 1.5, 1]))), str(rtg2)\n",
        "rtg3 = reward_to_go(torch.tensor([]), torch.tensor([]), 1)\n",
        "assert torch.all(torch.eq(rtg3, torch.tensor([])))\n",
        "rtg4 = reward_to_go(torch.tensor([1]), dones_1, 0.5)\n",
        "assert torch.eq(rtg4, torch.tensor(1)), str(rtg4)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BiCR784255h"
      },
      "source": [
        "def estimate_advantage(states:Tensor, rewards:Tensor, dones:Tensor, value_net:nn.Module, gamma:float, final_state:Tensor=None) -> Tensor:\n",
        "    '''\n",
        "        Compute advantage estimate for each step in a trajectory\n",
        "\n",
        "        Args:\n",
        "            rewards: (T, 1) torch.float32 - Reward given by each step in the trajectory\n",
        "            states: (T, state_size) torch.float32 - observation vectors for each state\n",
        "            dones: (T, 1) bool - if state was terminal\n",
        "            v_net: Trainable network which predicts the value V(s) of a state\n",
        "            gamma: Discount factor. Assume lambda = 1 from GAE-Lambda\n",
        "            final_state: if given final_state, dones[-1] must be equal to 0\n",
        "        Returns:\n",
        "            advantages: The estimated advantage for each step in the trajectory\n",
        "    '''\n",
        "\n",
        "    values = value_net(states) # Shape (T, 1)\n",
        "    values.masked_fill_(dones, 0)\n",
        "    rewards_to_go = reward_to_go(rewards, dones, gamma)\n",
        "    discounted_final_values = torch.zeros_like(values, dtype=torch.float32)\n",
        "    if final_state is not None:\n",
        "        final_value = value_net(final_state).item()\n",
        "        discount = gamma\n",
        "        for t in reversed(range(len(dones))):\n",
        "            if dones[t]:\n",
        "                break\n",
        "            discounted_final_values[t] = discount * final_value \n",
        "            discount *= gamma\n",
        "    advantages = rewards_to_go - values + discounted_final_values\n",
        "    return advantages"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "    This cell just holds some tests for the advantage_estimator() function\n",
        "'''\n",
        "class FakeValue(nn.Module):\n",
        "    '''\n",
        "        torch.nn.Module that returns the identiy. Useful for testing\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "fake_value_net = FakeValue()\n",
        "r = torch.tensor([1,1])\n",
        "s = torch.tensor([1,1])\n",
        "d = torch.tensor([0,0])\n",
        "\n",
        "# Test that discounting of values works with gamma = 1, done = True\n",
        "adv = estimate_advantage(s, r, d, fake_value_net, 1)\n",
        "assert torch.allclose(adv, torch.tensor([1,0], dtype=torch.float32)), adv\n",
        "# Test that discounting of values works with gamma = 0.3, done = True\n",
        "adv2 = estimate_advantage(s, r, d, fake_value_net, 0.3)\n",
        "assert torch.allclose(adv2, torch.tensor([0.3,0], dtype=torch.float32)), adv2\n",
        "# Test that this works with a final_state\n",
        "final_state = torch.tensor([2])\n",
        "adv3 = estimate_advantage(s, r, d, fake_value_net, 0.5, final_state)\n",
        "assert torch.allclose(adv3, torch.tensor([1,1], dtype=torch.float32)), adv3\n",
        "\n",
        "# Test that this works with intermediate dones:\n",
        "r = torch.tensor([1,1,0,1,1,0])\n",
        "s = torch.tensor([2,2,1,2,2,1])\n",
        "d = torch.tensor([0,0,1,0,0,1])\n",
        "adv4 = estimate_advantage(s, r, d, fake_value_net, 0.5, final_state=None)\n",
        "expected = torch.tensor([-0.5, -1, 0, -0.5, -1, 0])\n",
        "assert torch.allclose(adv4, expected), adv4"
      ],
      "metadata": {
        "id": "hVYMlPxmJSD_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikqk4irB2AIG"
      },
      "source": [
        "Step 5: Define the clipped loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAIH_Kur4gzl"
      },
      "source": [
        "def clipped_loss(states:Tensor, actions:Tensor, advantages:Tensor, pi_old:Tensor, pi_net:nn.Module, epsilon=0.2) -> Tensor:\n",
        "    '''\n",
        "        PPO Clipped Loss\n",
        "\n",
        "        Args:\n",
        "            states: The states from a given trajectory T\n",
        "            advantages: Advantage estimates for T, based on GAE-Lambda\n",
        "            pi_old: Probability distribution ~probabilities~ for pi(a|s) used to generate the trajectory\n",
        "            pi_net: Most up to date policy network. Provides LOGITS not probabilites\n",
        "            epsilon: Clipping hyperparameter for loss. See page 3 - https://arxiv.org/pdf/1707.06347.pdf\n",
        "        Returns:\n",
        "            loss: L_clip used to optimize the policy network\n",
        "    '''\n",
        "    pi_logits = pi_net(states)\n",
        "    pi = nn.functional.softmax(pi_logits, dim=1).gather(1, actions)\n",
        "    ratio = torch.div(pi, pi_old)\n",
        "    unclipped = ratio * advantages\n",
        "    clipped = torch.clamp(ratio, 1-epsilon, 1+epsilon) * advantages\n",
        "    elementwise_mins = torch.minimum(unclipped, clipped)\n",
        "    loss = -1 * torch.mean(elementwise_mins)\n",
        "    return loss"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV6r7gklSAho"
      },
      "source": [
        "Step 6: Main Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtUZi_RW-10N",
        "outputId": "d48d5e69-3e6e-4c0e-ec52-ea8d239cf030"
      },
      "source": [
        "# Set up the environment\n",
        "env = gym.make('CartPole-v0')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Build the neural networks and optimizers\n",
        "pi = MLP(state_size, action_size, hidden_size=32)\n",
        "v = MLP(state_size, 1, hidden_size=32, output_scale=10) # Values for this problem are roughly 0 - 200, so scale the initial 0-1\n",
        "\n",
        "pi_optimizer = torch.optim.Adam(pi.parameters(), lr=1e-2)\n",
        "v_optimizer = torch.optim.Adam(v.parameters(), lr=1e-2)\n",
        "v_loss_fn = nn.MSELoss()\n",
        "\n",
        "# Set up the buffer and actor\n",
        "buffer = ReplayBuffer()\n",
        "actor = Actor(env, buffer, pi)\n",
        "\n",
        "# Hyperparameters\n",
        "total_epochs = 101\n",
        "episode = 0\n",
        "longest_episode_length = 0\n",
        "gamma = 0.95\n",
        "K_pi = 3\n",
        "K_v = 3\n",
        "minibatch_size = 100\n",
        "explore_steps = 1000\n",
        "epsilon = 0.2\n",
        "\n",
        "\n",
        "for epoch in range(total_epochs):\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'epoch: {epoch}')\n",
        "\n",
        "    # Play steps to generate on-policy data\n",
        "    episode_length = 0\n",
        "    episodes_this_epoch = 0\n",
        "    for _ in range(explore_steps):\n",
        "        done = actor.play_step()\n",
        "        episode_length += 1\n",
        "        if done:\n",
        "            episode += 1\n",
        "            print(f'episode: {episode} length: {episode_length}')\n",
        "            episodes_this_epoch += 1\n",
        "            # Log eisode information to the console\n",
        "            if episode_length >= longest_episode_length:\n",
        "                longest_episode_length = episode_length\n",
        "                #print(f'epoch: {epoch} episode: {episode} length: {longest_episode_length}')\n",
        "            episode_length = 0\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        print(f'average episode length: {explore_steps / episodes_this_epoch}')\n",
        "\n",
        "\n",
        "    # Unpack replay buffer into arrays and reset the buffer\n",
        "    states, actions, rewards, dones, new_states = buffer.to_batch()\n",
        "    actor.reset()\n",
        "\n",
        "    # Compute the information we'll need to feed the two loss functions\n",
        "    if done:\n",
        "        final_state = None\n",
        "    else:\n",
        "        final_state = new_states[-1]\n",
        "    advantages = estimate_advantage(states, rewards, dones, v, gamma, final_state).detach()\n",
        "    pi_old_logits = pi(states)\n",
        "    pi_old = nn.functional.softmax(pi_old_logits, dim=1).gather(1, actions).detach()\n",
        "    rtg = reward_to_go(rewards, dones, gamma).detach()\n",
        "\n",
        "    # Update policy network for K steps\n",
        "    for _ in range(K_pi):\n",
        "        for i in range(0, len(states), minibatch_size):\n",
        "            pi_optimizer.zero_grad()\n",
        "            pi_loss = clipped_loss(states[i:i+minibatch_size], actions[i:i+minibatch_size], advantages[i:i+minibatch_size], pi_old[i:i+minibatch_size], pi, epsilon=epsilon)\n",
        "            pi_loss.backward()\n",
        "            pi_optimizer.step()\n",
        "\n",
        "    # Update Value Network V(s_t) should be reward_to_go_t. MSE Loss\n",
        "    for _ in range(K_v):\n",
        "        for i in range(0, len(states), minibatch_size):\n",
        "            values = v(states[i:i+minibatch_size])\n",
        "            v_optimizer.zero_grad()\n",
        "            v_loss = v_loss_fn(values, rtg[i:i+minibatch_size])\n",
        "            v_loss.backward()\n",
        "            v_optimizer.step()    \n"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "episode: 1 length: 29\n",
            "episode: 2 length: 19\n",
            "episode: 3 length: 31\n",
            "episode: 4 length: 28\n",
            "episode: 5 length: 12\n",
            "episode: 6 length: 15\n",
            "episode: 7 length: 9\n",
            "episode: 8 length: 24\n",
            "episode: 9 length: 14\n",
            "episode: 10 length: 43\n",
            "episode: 11 length: 29\n",
            "episode: 12 length: 17\n",
            "episode: 13 length: 17\n",
            "episode: 14 length: 14\n",
            "episode: 15 length: 16\n",
            "episode: 16 length: 15\n",
            "episode: 17 length: 24\n",
            "episode: 18 length: 15\n",
            "episode: 19 length: 28\n",
            "episode: 20 length: 15\n",
            "episode: 21 length: 17\n",
            "episode: 22 length: 11\n",
            "episode: 23 length: 14\n",
            "episode: 24 length: 24\n",
            "episode: 25 length: 72\n",
            "episode: 26 length: 8\n",
            "episode: 27 length: 15\n",
            "episode: 28 length: 28\n",
            "episode: 29 length: 31\n",
            "episode: 30 length: 32\n",
            "episode: 31 length: 24\n",
            "episode: 32 length: 43\n",
            "episode: 33 length: 23\n",
            "episode: 34 length: 19\n",
            "episode: 35 length: 39\n",
            "episode: 36 length: 12\n",
            "episode: 37 length: 11\n",
            "episode: 38 length: 20\n",
            "episode: 39 length: 22\n",
            "episode: 40 length: 12\n",
            "episode: 41 length: 9\n",
            "episode: 42 length: 29\n",
            "episode: 43 length: 20\n",
            "episode: 44 length: 15\n",
            "episode: 45 length: 12\n",
            "episode: 46 length: 13\n",
            "average episode length: 21.73913043478261\n",
            "episode: 47 length: 12\n",
            "episode: 48 length: 21\n",
            "episode: 49 length: 9\n",
            "episode: 50 length: 58\n",
            "episode: 51 length: 14\n",
            "episode: 52 length: 19\n",
            "episode: 53 length: 17\n",
            "episode: 54 length: 57\n",
            "episode: 55 length: 13\n",
            "episode: 56 length: 50\n",
            "episode: 57 length: 19\n",
            "episode: 58 length: 28\n",
            "episode: 59 length: 53\n",
            "episode: 60 length: 15\n",
            "episode: 61 length: 44\n",
            "episode: 62 length: 41\n",
            "episode: 63 length: 32\n",
            "episode: 64 length: 24\n",
            "episode: 65 length: 16\n",
            "episode: 66 length: 16\n",
            "episode: 67 length: 18\n",
            "episode: 68 length: 18\n",
            "episode: 69 length: 39\n",
            "episode: 70 length: 37\n",
            "episode: 71 length: 64\n",
            "episode: 72 length: 24\n",
            "episode: 73 length: 20\n",
            "episode: 74 length: 101\n",
            "episode: 75 length: 20\n",
            "episode: 76 length: 34\n",
            "episode: 77 length: 12\n",
            "episode: 78 length: 12\n",
            "episode: 79 length: 34\n",
            "episode: 80 length: 81\n",
            "episode: 81 length: 17\n",
            "episode: 82 length: 64\n",
            "episode: 83 length: 29\n",
            "episode: 84 length: 40\n",
            "episode: 85 length: 58\n",
            "episode: 86 length: 31\n",
            "episode: 87 length: 45\n",
            "episode: 88 length: 70\n",
            "episode: 89 length: 80\n",
            "episode: 90 length: 70\n",
            "episode: 91 length: 39\n",
            "episode: 92 length: 114\n",
            "episode: 93 length: 61\n",
            "episode: 94 length: 55\n",
            "episode: 95 length: 21\n",
            "episode: 96 length: 17\n",
            "episode: 97 length: 36\n",
            "episode: 98 length: 29\n",
            "episode: 99 length: 137\n",
            "episode: 100 length: 59\n",
            "episode: 101 length: 77\n",
            "episode: 102 length: 63\n",
            "episode: 103 length: 36\n",
            "episode: 104 length: 163\n",
            "episode: 105 length: 157\n",
            "episode: 106 length: 61\n",
            "episode: 107 length: 121\n",
            "episode: 108 length: 71\n",
            "episode: 109 length: 30\n",
            "episode: 110 length: 98\n",
            "episode: 111 length: 200\n",
            "episode: 112 length: 21\n",
            "episode: 113 length: 123\n",
            "episode: 114 length: 87\n",
            "episode: 115 length: 150\n",
            "episode: 116 length: 28\n",
            "episode: 117 length: 200\n",
            "episode: 118 length: 155\n",
            "episode: 119 length: 200\n",
            "episode: 120 length: 167\n",
            "episode: 121 length: 192\n",
            "episode: 122 length: 158\n",
            "episode: 123 length: 182\n",
            "episode: 124 length: 166\n",
            "episode: 125 length: 185\n",
            "episode: 126 length: 176\n",
            "episode: 127 length: 42\n",
            "episode: 128 length: 195\n",
            "episode: 129 length: 200\n",
            "episode: 130 length: 200\n",
            "episode: 131 length: 197\n",
            "episode: 132 length: 155\n",
            "episode: 133 length: 200\n",
            "episode: 134 length: 200\n",
            "episode: 135 length: 177\n",
            "episode: 136 length: 200\n",
            "episode: 137 length: 200\n",
            "episode: 138 length: 194\n",
            "episode: 139 length: 200\n",
            "episode: 140 length: 200\n",
            "episode: 141 length: 200\n",
            "episode: 142 length: 174\n",
            "episode: 143 length: 200\n",
            "epoch: 10\n",
            "episode: 144 length: 154\n",
            "episode: 145 length: 179\n",
            "episode: 146 length: 200\n",
            "episode: 147 length: 200\n",
            "episode: 148 length: 177\n",
            "average episode length: 200.0\n",
            "episode: 149 length: 167\n",
            "episode: 150 length: 200\n",
            "episode: 151 length: 200\n",
            "episode: 152 length: 185\n",
            "episode: 153 length: 200\n",
            "episode: 154 length: 155\n",
            "episode: 155 length: 135\n",
            "episode: 156 length: 147\n",
            "episode: 157 length: 141\n",
            "episode: 158 length: 68\n",
            "episode: 159 length: 127\n",
            "episode: 160 length: 133\n",
            "episode: 161 length: 158\n",
            "episode: 162 length: 187\n",
            "episode: 163 length: 200\n",
            "episode: 164 length: 171\n",
            "episode: 165 length: 200\n",
            "episode: 166 length: 177\n",
            "episode: 167 length: 134\n",
            "episode: 168 length: 177\n",
            "episode: 169 length: 184\n",
            "episode: 170 length: 143\n",
            "episode: 171 length: 168\n",
            "episode: 172 length: 178\n",
            "episode: 173 length: 157\n",
            "episode: 174 length: 176\n",
            "episode: 175 length: 156\n",
            "episode: 176 length: 186\n",
            "episode: 177 length: 148\n",
            "episode: 178 length: 199\n",
            "episode: 179 length: 147\n",
            "episode: 180 length: 148\n",
            "episode: 181 length: 154\n",
            "episode: 182 length: 174\n",
            "episode: 183 length: 200\n",
            "episode: 184 length: 192\n",
            "episode: 185 length: 194\n",
            "episode: 186 length: 173\n",
            "episode: 187 length: 195\n",
            "episode: 188 length: 177\n",
            "episode: 189 length: 200\n",
            "episode: 190 length: 171\n",
            "episode: 191 length: 171\n",
            "episode: 192 length: 200\n",
            "episode: 193 length: 200\n",
            "episode: 194 length: 184\n",
            "episode: 195 length: 200\n",
            "episode: 196 length: 200\n",
            "episode: 197 length: 195\n",
            "epoch: 20\n",
            "episode: 198 length: 195\n",
            "episode: 199 length: 200\n",
            "episode: 200 length: 200\n",
            "episode: 201 length: 200\n",
            "episode: 202 length: 200\n",
            "average episode length: 200.0\n",
            "episode: 203 length: 200\n",
            "episode: 204 length: 200\n",
            "episode: 205 length: 200\n",
            "episode: 206 length: 200\n",
            "episode: 207 length: 200\n",
            "episode: 208 length: 200\n",
            "episode: 209 length: 200\n",
            "episode: 210 length: 200\n",
            "episode: 211 length: 200\n",
            "episode: 212 length: 200\n",
            "episode: 213 length: 200\n",
            "episode: 214 length: 200\n",
            "episode: 215 length: 200\n",
            "episode: 216 length: 200\n",
            "episode: 217 length: 200\n",
            "episode: 218 length: 200\n",
            "episode: 219 length: 200\n",
            "episode: 220 length: 200\n",
            "episode: 221 length: 200\n",
            "episode: 222 length: 200\n",
            "episode: 223 length: 200\n",
            "episode: 224 length: 200\n",
            "episode: 225 length: 200\n",
            "episode: 226 length: 200\n",
            "episode: 227 length: 200\n",
            "episode: 228 length: 200\n",
            "episode: 229 length: 200\n",
            "episode: 230 length: 200\n",
            "episode: 231 length: 200\n",
            "episode: 232 length: 200\n",
            "episode: 233 length: 200\n",
            "episode: 234 length: 200\n",
            "episode: 235 length: 200\n",
            "episode: 236 length: 200\n",
            "episode: 237 length: 200\n",
            "episode: 238 length: 200\n",
            "episode: 239 length: 200\n",
            "episode: 240 length: 200\n",
            "episode: 241 length: 200\n",
            "episode: 242 length: 200\n",
            "episode: 243 length: 200\n",
            "episode: 244 length: 200\n",
            "episode: 245 length: 200\n",
            "episode: 246 length: 200\n",
            "episode: 247 length: 200\n",
            "epoch: 30\n",
            "episode: 248 length: 200\n",
            "episode: 249 length: 200\n",
            "episode: 250 length: 200\n",
            "episode: 251 length: 200\n",
            "episode: 252 length: 200\n",
            "average episode length: 200.0\n",
            "episode: 253 length: 200\n",
            "episode: 254 length: 200\n",
            "episode: 255 length: 200\n",
            "episode: 256 length: 200\n",
            "episode: 257 length: 200\n",
            "episode: 258 length: 200\n",
            "episode: 259 length: 200\n",
            "episode: 260 length: 200\n",
            "episode: 261 length: 200\n",
            "episode: 262 length: 200\n",
            "episode: 263 length: 200\n",
            "episode: 264 length: 200\n",
            "episode: 265 length: 200\n",
            "episode: 266 length: 200\n",
            "episode: 267 length: 200\n",
            "episode: 268 length: 200\n",
            "episode: 269 length: 200\n",
            "episode: 270 length: 200\n",
            "episode: 271 length: 200\n",
            "episode: 272 length: 200\n",
            "episode: 273 length: 200\n",
            "episode: 274 length: 200\n",
            "episode: 275 length: 200\n",
            "episode: 276 length: 200\n",
            "episode: 277 length: 200\n",
            "episode: 278 length: 200\n",
            "episode: 279 length: 200\n",
            "episode: 280 length: 200\n",
            "episode: 281 length: 200\n",
            "episode: 282 length: 200\n",
            "episode: 283 length: 200\n",
            "episode: 284 length: 200\n",
            "episode: 285 length: 200\n",
            "episode: 286 length: 200\n",
            "episode: 287 length: 200\n",
            "episode: 288 length: 200\n",
            "episode: 289 length: 200\n",
            "episode: 290 length: 200\n",
            "episode: 291 length: 200\n",
            "episode: 292 length: 200\n",
            "episode: 293 length: 200\n",
            "episode: 294 length: 200\n",
            "episode: 295 length: 200\n",
            "episode: 296 length: 200\n",
            "episode: 297 length: 200\n",
            "epoch: 40\n",
            "episode: 298 length: 200\n",
            "episode: 299 length: 200\n",
            "episode: 300 length: 200\n",
            "episode: 301 length: 200\n",
            "episode: 302 length: 200\n",
            "average episode length: 200.0\n",
            "episode: 303 length: 200\n",
            "episode: 304 length: 200\n",
            "episode: 305 length: 200\n",
            "episode: 306 length: 200\n",
            "episode: 307 length: 200\n",
            "episode: 308 length: 200\n",
            "episode: 309 length: 200\n",
            "episode: 310 length: 200\n",
            "episode: 311 length: 200\n",
            "episode: 312 length: 200\n",
            "episode: 313 length: 200\n",
            "episode: 314 length: 200\n",
            "episode: 315 length: 145\n",
            "episode: 316 length: 200\n",
            "episode: 317 length: 200\n",
            "episode: 318 length: 200\n",
            "episode: 319 length: 200\n",
            "episode: 320 length: 178\n",
            "episode: 321 length: 200\n",
            "episode: 322 length: 200\n",
            "episode: 323 length: 200\n",
            "episode: 324 length: 200\n",
            "episode: 325 length: 200\n",
            "episode: 326 length: 200\n",
            "episode: 327 length: 29\n",
            "episode: 328 length: 200\n",
            "episode: 329 length: 200\n",
            "episode: 330 length: 200\n",
            "episode: 331 length: 200\n",
            "episode: 332 length: 200\n",
            "episode: 333 length: 200\n",
            "episode: 334 length: 200\n",
            "episode: 335 length: 200\n",
            "episode: 336 length: 200\n",
            "episode: 337 length: 200\n",
            "episode: 338 length: 200\n",
            "episode: 339 length: 200\n",
            "episode: 340 length: 200\n",
            "episode: 341 length: 200\n",
            "episode: 342 length: 200\n",
            "episode: 343 length: 200\n",
            "episode: 344 length: 200\n",
            "episode: 345 length: 200\n",
            "episode: 346 length: 200\n",
            "episode: 347 length: 183\n",
            "epoch: 50\n",
            "episode: 348 length: 186\n",
            "episode: 349 length: 166\n",
            "episode: 350 length: 200\n",
            "episode: 351 length: 188\n",
            "episode: 352 length: 200\n",
            "average episode length: 200.0\n",
            "episode: 353 length: 200\n",
            "episode: 354 length: 200\n",
            "episode: 355 length: 200\n",
            "episode: 356 length: 171\n",
            "episode: 357 length: 198\n",
            "episode: 358 length: 200\n",
            "episode: 359 length: 200\n",
            "episode: 360 length: 200\n",
            "episode: 361 length: 200\n",
            "episode: 362 length: 200\n",
            "episode: 363 length: 200\n",
            "episode: 364 length: 200\n",
            "episode: 365 length: 200\n",
            "episode: 366 length: 200\n",
            "episode: 367 length: 200\n",
            "episode: 368 length: 200\n",
            "episode: 369 length: 168\n",
            "episode: 370 length: 200\n",
            "episode: 371 length: 163\n",
            "episode: 372 length: 188\n",
            "episode: 373 length: 178\n",
            "episode: 374 length: 170\n",
            "episode: 375 length: 154\n",
            "episode: 376 length: 170\n",
            "episode: 377 length: 200\n",
            "episode: 378 length: 174\n",
            "episode: 379 length: 192\n",
            "episode: 380 length: 200\n",
            "episode: 381 length: 141\n",
            "episode: 382 length: 158\n",
            "episode: 383 length: 165\n",
            "episode: 384 length: 200\n",
            "episode: 385 length: 166\n",
            "episode: 386 length: 200\n",
            "episode: 387 length: 158\n",
            "episode: 388 length: 151\n",
            "episode: 389 length: 156\n",
            "episode: 390 length: 129\n",
            "episode: 391 length: 200\n",
            "episode: 392 length: 200\n",
            "episode: 393 length: 200\n",
            "episode: 394 length: 179\n",
            "episode: 395 length: 165\n",
            "episode: 396 length: 200\n",
            "episode: 397 length: 200\n",
            "epoch: 60\n",
            "episode: 398 length: 164\n",
            "episode: 399 length: 131\n",
            "episode: 400 length: 149\n",
            "episode: 401 length: 143\n",
            "episode: 402 length: 162\n",
            "episode: 403 length: 117\n",
            "average episode length: 166.66666666666666\n",
            "episode: 404 length: 159\n",
            "episode: 405 length: 135\n",
            "episode: 406 length: 178\n",
            "episode: 407 length: 133\n",
            "episode: 408 length: 200\n",
            "episode: 409 length: 200\n",
            "episode: 410 length: 191\n",
            "episode: 411 length: 145\n",
            "episode: 412 length: 200\n",
            "episode: 413 length: 197\n",
            "episode: 414 length: 181\n",
            "episode: 415 length: 157\n",
            "episode: 416 length: 178\n",
            "episode: 417 length: 139\n",
            "episode: 418 length: 200\n",
            "episode: 419 length: 200\n",
            "episode: 420 length: 200\n",
            "episode: 421 length: 200\n",
            "episode: 422 length: 164\n",
            "episode: 423 length: 200\n",
            "episode: 424 length: 162\n",
            "episode: 425 length: 200\n",
            "episode: 426 length: 153\n",
            "episode: 427 length: 162\n",
            "episode: 428 length: 200\n",
            "episode: 429 length: 168\n",
            "episode: 430 length: 200\n",
            "episode: 431 length: 200\n",
            "episode: 432 length: 197\n",
            "episode: 433 length: 200\n",
            "episode: 434 length: 154\n",
            "episode: 435 length: 155\n",
            "episode: 436 length: 182\n",
            "episode: 437 length: 176\n",
            "episode: 438 length: 157\n",
            "episode: 439 length: 147\n",
            "episode: 440 length: 177\n",
            "episode: 441 length: 200\n",
            "episode: 442 length: 179\n",
            "episode: 443 length: 176\n",
            "episode: 444 length: 179\n",
            "episode: 445 length: 200\n",
            "episode: 446 length: 158\n",
            "episode: 447 length: 200\n",
            "episode: 448 length: 146\n",
            "episode: 449 length: 188\n",
            "epoch: 70\n",
            "episode: 450 length: 142\n",
            "episode: 451 length: 200\n",
            "episode: 452 length: 200\n",
            "episode: 453 length: 147\n",
            "episode: 454 length: 200\n",
            "average episode length: 200.0\n",
            "episode: 455 length: 192\n",
            "episode: 456 length: 165\n",
            "episode: 457 length: 172\n",
            "episode: 458 length: 167\n",
            "episode: 459 length: 155\n",
            "episode: 460 length: 147\n",
            "episode: 461 length: 146\n",
            "episode: 462 length: 137\n",
            "episode: 463 length: 200\n",
            "episode: 464 length: 187\n",
            "episode: 465 length: 184\n",
            "episode: 466 length: 163\n",
            "episode: 467 length: 149\n",
            "episode: 468 length: 200\n",
            "episode: 469 length: 180\n",
            "episode: 470 length: 181\n",
            "episode: 471 length: 160\n",
            "episode: 472 length: 160\n",
            "episode: 473 length: 156\n",
            "episode: 474 length: 149\n",
            "episode: 475 length: 171\n",
            "episode: 476 length: 200\n",
            "episode: 477 length: 200\n",
            "episode: 478 length: 200\n",
            "episode: 479 length: 200\n",
            "episode: 480 length: 200\n",
            "episode: 481 length: 200\n",
            "episode: 482 length: 200\n",
            "episode: 483 length: 200\n",
            "episode: 484 length: 200\n",
            "episode: 485 length: 200\n",
            "episode: 486 length: 176\n",
            "episode: 487 length: 145\n",
            "episode: 488 length: 188\n",
            "episode: 489 length: 124\n",
            "episode: 490 length: 200\n",
            "episode: 491 length: 200\n",
            "episode: 492 length: 200\n",
            "episode: 493 length: 165\n",
            "episode: 494 length: 173\n",
            "episode: 495 length: 200\n",
            "episode: 496 length: 148\n",
            "episode: 497 length: 159\n",
            "episode: 498 length: 200\n",
            "episode: 499 length: 200\n",
            "episode: 500 length: 200\n",
            "episode: 501 length: 200\n",
            "epoch: 80\n",
            "episode: 502 length: 191\n",
            "episode: 503 length: 173\n",
            "episode: 504 length: 200\n",
            "episode: 505 length: 200\n",
            "episode: 506 length: 160\n",
            "average episode length: 200.0\n",
            "episode: 507 length: 200\n",
            "episode: 508 length: 139\n",
            "episode: 509 length: 200\n",
            "episode: 510 length: 200\n",
            "episode: 511 length: 185\n",
            "episode: 512 length: 200\n",
            "episode: 513 length: 200\n",
            "episode: 514 length: 200\n",
            "episode: 515 length: 200\n",
            "episode: 516 length: 200\n",
            "episode: 517 length: 173\n",
            "episode: 518 length: 200\n",
            "episode: 519 length: 200\n",
            "episode: 520 length: 200\n",
            "episode: 521 length: 175\n",
            "episode: 522 length: 200\n",
            "episode: 523 length: 200\n",
            "episode: 524 length: 200\n",
            "episode: 525 length: 200\n",
            "episode: 526 length: 200\n",
            "episode: 527 length: 184\n",
            "episode: 528 length: 167\n",
            "episode: 529 length: 200\n",
            "episode: 530 length: 200\n",
            "episode: 531 length: 200\n",
            "episode: 532 length: 200\n",
            "episode: 533 length: 200\n",
            "episode: 534 length: 200\n",
            "episode: 535 length: 200\n",
            "episode: 536 length: 196\n",
            "episode: 537 length: 200\n",
            "episode: 538 length: 197\n",
            "episode: 539 length: 200\n",
            "episode: 540 length: 200\n",
            "episode: 541 length: 154\n",
            "episode: 542 length: 155\n",
            "episode: 543 length: 134\n",
            "episode: 544 length: 200\n",
            "episode: 545 length: 200\n",
            "episode: 546 length: 154\n",
            "episode: 547 length: 153\n",
            "episode: 548 length: 174\n",
            "episode: 549 length: 200\n",
            "episode: 550 length: 157\n",
            "episode: 551 length: 152\n",
            "episode: 552 length: 157\n",
            "epoch: 90\n",
            "episode: 553 length: 183\n",
            "episode: 554 length: 158\n",
            "episode: 555 length: 189\n",
            "episode: 556 length: 156\n",
            "episode: 557 length: 200\n",
            "average episode length: 200.0\n",
            "episode: 558 length: 166\n",
            "episode: 559 length: 188\n",
            "episode: 560 length: 200\n",
            "episode: 561 length: 198\n",
            "episode: 562 length: 200\n",
            "episode: 563 length: 200\n",
            "episode: 564 length: 182\n",
            "episode: 565 length: 196\n",
            "episode: 566 length: 131\n",
            "episode: 567 length: 178\n",
            "episode: 568 length: 152\n",
            "episode: 569 length: 172\n",
            "episode: 570 length: 128\n",
            "episode: 571 length: 200\n",
            "episode: 572 length: 131\n",
            "episode: 573 length: 169\n",
            "episode: 574 length: 156\n",
            "episode: 575 length: 200\n",
            "episode: 576 length: 131\n",
            "episode: 577 length: 200\n",
            "episode: 578 length: 163\n",
            "episode: 579 length: 200\n",
            "episode: 580 length: 150\n",
            "episode: 581 length: 138\n",
            "episode: 582 length: 168\n",
            "episode: 583 length: 185\n",
            "episode: 584 length: 167\n",
            "episode: 585 length: 192\n",
            "episode: 586 length: 190\n",
            "episode: 587 length: 161\n",
            "episode: 588 length: 200\n",
            "episode: 589 length: 200\n",
            "episode: 590 length: 200\n",
            "episode: 591 length: 179\n",
            "episode: 592 length: 186\n",
            "episode: 593 length: 150\n",
            "episode: 594 length: 163\n",
            "episode: 595 length: 200\n",
            "episode: 596 length: 173\n",
            "episode: 597 length: 197\n",
            "episode: 598 length: 172\n",
            "episode: 599 length: 189\n",
            "episode: 600 length: 185\n",
            "episode: 601 length: 200\n",
            "episode: 602 length: 156\n",
            "episode: 603 length: 191\n",
            "epoch: 100\n",
            "episode: 604 length: 200\n",
            "episode: 605 length: 200\n",
            "episode: 606 length: 179\n",
            "episode: 607 length: 156\n",
            "episode: 608 length: 199\n",
            "average episode length: 200.0\n"
          ]
        }
      ]
    }
  ]
}